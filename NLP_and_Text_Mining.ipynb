{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DeepLearning= \"\"\"Deep learning is an artificial intelligence (AI) function that imitates the workings of the human brain in processing data and \n",
    "creating patterns for use in decision making. \n",
    "Deep learning is a subset of machine learning in artificial intelligence that has networks capable of learning unsupervised from data that is\n",
    "unstructured or unlabeled. \n",
    "Also known as deep neural learning or deep neural network. \n",
    "\n",
    "Deep learning has evolved hand-in-hand with the digital era, which has brought about an explosion of data in all forms and from every\n",
    "region of the world. This data, known simply as big data, is drawn from sources like social media, internet search engines, e-commerce platforms, \n",
    "and online cinemas, among others. This enormous\n",
    "amount of data is readily accessible and can be shared through fintech applications like cloud computing.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(DeepLearning)\n",
    "len(DeepLearning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "DL_tokens= word_tokenize(DeepLearning)\n",
    "DL_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(DL_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for words in DL_tokens:\n",
    "    count[words.lower()]+=1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_top15=count.most_common(15)\n",
    "count_top15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import blankline_tokenize\n",
    "DL_blnklin=blankline_tokenize(DeepLearning)\n",
    "len(DL_blnklin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams, bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL_bigrams= list(nltk.bigrams(DL_tokens))\n",
    "DL_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL_trigrams= list(nltk.trigrams(DL_tokens))\n",
    "DL_trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The two popular stemming algorithms: Porter Stemmer and Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "DL_pst = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for words in DL_tokens:\n",
    "    print(words + \":\" + DL_pst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "DL_lancast = LancasterStemmer()\n",
    "for words in DL_tokens:\n",
    "    print(words + \":\" + DL_lancast.stem(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "DL_Lem=WordNetLemmatizer()\n",
    "\n",
    "for words in DL_tokens:\n",
    "    print(words+ \":\" +DL_Lem.lemmatize(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern=re.compile(r'[-;,:.!?0-9()|]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pattern=[]\n",
    "for words in DL_tokens:\n",
    "    text=pattern.sub(\"\",words)\n",
    "    if len(word)>0:\n",
    "        text_pattern.append(text)\n",
    "text_pattern   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_1 = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "DeepLearning_new = pattern.sub('', DeepLearning)\n",
    "DeepLearning_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(DeepLearning_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL_tokens_n= word_tokenize(DeepLearning_new)\n",
    "DL_tokens_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts-Of-Speech (POS) tagging\n",
    "To identify whether the word is a noun, adjective, verb, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "for token in DL_tokens_n:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the text into meaningful chunks (sentences)\n",
    "The PunktSentenceTokenizer is an unsupervised trainable model. This means it can be trained on unlabeled text. An Unlabeled text is a written work that is not split into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "nltk.download('state_union')\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train = state_union.raw(\"2005-GWBush.txt\")\n",
    "test = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "sent_tokenizer = PunktSentenceTokenizer(train)\n",
    "tokenized = sent_tokenizer.tokenize(test)\n",
    "\n",
    "def preprocess():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            token = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(token)\n",
    "            chunk_Gram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunk_Parser = nltk.RegexpParser(chunk_Gram)\n",
    "            chunked = chunk_Parser.parse(tagged)\n",
    "            \n",
    "            print(chunked)\n",
    "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "\n",
    "            chunked.draw()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tokenizer on a piece of text\n",
    "sentences = \"Prof. Johnson accepted to accompany Ms. Smith to the theatre. They spent enjoyable time together.\"\n",
    " \n",
    "print(custom_sent_tokenizer.tokenize(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add more abbreviations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the learned abbreviations from the trainin text\n",
    "print(custom_sent_tokenizer._params.abbrev_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add more abbreviations\n",
    "custom_sent_tokenizer._params.abbrev_types.add('ms')\n",
    "print(custom_sent_tokenizer._params.abbrev_types)\n",
    "print(custom_sent_tokenizer.tokenize(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktParameters\n",
    "punktP = PunktParameters()\n",
    "punktP.abbrev_types = set(['mr', 'mrs', 'ms', 'dr', 'prof'])\n",
    "sentence_split = PunktSentenceTokenizer(punktP)\n",
    "new_text = sentence_split.tokenize(sentences)\n",
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "my_text=nlp(DeepLearning)\n",
    "\n",
    "# displaying tokens with their POS tags\n",
    "displacy.render(my_text,style='dep',jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
